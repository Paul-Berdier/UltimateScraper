# ğŸ•¸ï¸ UltimateScraper â€” The Ultimate Multilingual, Keyword-Driven Web Crawler

### *Massive automated data collection for domain-specific AI training (Wine, Cosmetics, Fraud, etc.)*

UltimateScraper est un **crawler / scraper intelligent**, modulaire et paramÃ©trable, conÃ§u pour :

* dÃ©couvrir automatiquement de nouveaux sites Ã  partir de **mots-clÃ©s** ;
* explorer le Web dans plusieurs **langues** ;
* filtrer les pages par **pertinence sÃ©mantique** (embeddings multilingues) ;
* respecter des limites strictes (**mÃ©moire**, **pages**, **domaines**) ;
* produire un **corpus propre**, prÃªt pour lâ€™entraÃ®nement de modÃ¨les IA.

Il ne dÃ©pend dâ€™aucun moteur externe : lâ€™utilisateur fournit simplement des **keywords**, des **langues**, et des **contraintes**, et le systÃ¨me rÃ©cupÃ¨re *tout ce qui est pertinent* sur le Web.

Ce projet sert de fondation Ã  la construction de jeux de donnÃ©es massifs pour des modÃ¨les **encodeur-dÃ©codeur spÃ©cialisÃ©s** (ex. : vins, cosmÃ©tique, agriculture, produits sÃ©curisÃ©s).

---

# âœ¨ FonctionnalitÃ©s majeures

### ğŸ” Recherche intelligente de nouvelles sources

* EntrÃ©e : mots-clÃ©s + langues
* DÃ©couverte automatique de domaines Ã  explorer
* DÃ©duplication et normalisation des sites

### ğŸ•·ï¸ Crawler robuste

* Requests + Politeness Policier (delay configurable)
* Gestion robots.txt
* File dâ€™attente dâ€™URLs (Frontier BFS)
* Scheduler par domaine (limite pages/domaines)
* Mode 100% Python (pas besoin de Selenium)

### ğŸ§  Filtrage IA de la pertinence

* Embeddings multilingues (`sentence-transformers`)
* SimilaritÃ© cosinus entre mots-clÃ©s â†” contenu
* DÃ©tection de langue automatique
* Mode fallback "keyword relevance" disponible

### ğŸ§¹ Extraction de texte propre

* `trafilatura` pour nettoyer HTML â†’ texte
* Suppression du bruit, mÃ©tadonnÃ©es, structure Web

### ğŸ’¾ Stockage optimisÃ©

* Sorties en JSONL (raw + filtered)
* Writer rotatif en cas de gros volumes
* Counting mÃ©moire pour arrÃªter le job automatiquement

---

# ğŸ“‚ Structure du projet

```
ultimate-crawler/
â”œâ”€ README.md
â”œâ”€ pyproject.toml
â”œâ”€ requirements.txt
â”œâ”€ .gitignore
â”‚
â”œâ”€ configs/
â”‚  â”œâ”€ job_wine.yaml        # Exemple de job
â”‚  â””â”€ job_XXX.yaml         # Autres jobs
â”‚
â”œâ”€ data/
â”‚  â”œâ”€ jobs/                # Outputs par job
â”‚  â””â”€ cache/
â”‚     â”œâ”€ robots/           # Cache robots.txt
â”‚     â””â”€ embeddings/       # (optionnel) cache d'embeddings
â”‚
â”œâ”€ scripts/
â”‚  â”œâ”€ run_job.py           # Lance un job complet
â”‚  â””â”€ debug_single_url.py  # Tester toute la pipeline sur 1 URL
â”‚
â””â”€ src/
   â””â”€ ultimate_crawler/
      â”œâ”€ config/           # Chargement YAML + dataclasses
      â”œâ”€ discovery/        # Recherche de domaines
      â”œâ”€ crawl/            # Frontier, fetcher, scheduler, parser
      â”œâ”€ relevance/        # Embeddings, langue, filtres
      â”œâ”€ core/             # JobRunner + mÃ©triques
      â””â”€ io/               # Writers JSONL + logging
```

---

# ğŸš€ Installation

## 1. CrÃ©er un environnement virtuel

```bash
python -m venv .venv
source .venv/bin/activate   # Linux/Mac
.\.venv\Scripts\activate    # Windows PowerShell
```

## 2. Installer UltimateScraper

Ã€ la racine du projet :

```bash
pip install -e .
```

## 3. Installer les dÃ©pendances

```bash
pip install -r requirements.txt
```

---

# âš™ï¸ Configuration dâ€™un job

Les jobs sont dÃ©finis dans `configs/job_*.yaml`.

Exemple (`configs/job_wine.yaml`) :

```yaml
job_name: "wine_multilingual_crawl"

keywords:
  - "vin"
  - "wine"
  - "vino"
  - "è‘¡è„é…’"

languages: ["fr", "en", "es", "zh"]

limits:
  max_domains: 200
  max_pages: 20000
  memory_limit_mb: 10240
  max_pages_per_domain: 500

crawler:
  user_agent: "UltimateCrawler/1.0"
  request_timeout: 10
  obey_robots_txt: true
  politeness_delay: 0.5

relevance:
  min_chars: 400
  relevance_threshold: 0.35
  model: "embedding"
  embedding_model_name: "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"

output:
  dir: "data/jobs/wine_multilingual"
  raw_pages_file: "raw_pages.jsonl"
  filtered_docs_file: "docs_filtered.jsonl"

seeds:
  - "https://www.winefolly.com/"
  - "https://www.hachette-vins.com/"
```

---

# â–¶ï¸ Lancement dâ€™un job

Depuis la racine :

```bash
python scripts/run_job.py -c configs/job_wine.yaml
```

Sortie attendue :

```
=== Job: wine_multilingual_crawl ===
[INFO] Seeds found: 4
[INFO] Job finished.
```

Les fichiers gÃ©nÃ©rÃ©s :

```
data/jobs/wine_multilingual/
â”‚
â”œâ”€ raw_pages.jsonl
â””â”€ docs_filtered.jsonl
```

---

# ğŸ” Debug sur une URL unique

```bash
python scripts/debug_single_url.py -c configs/job_wine.yaml -u "https://www.winefolly.com/"
```

Affiche :

* dÃ©tected language
* extracted text length
* relevance score
* Ã©ventuels warnings

---

# ğŸ“„ Format de sortie

Chaque document est stockÃ© en JSONL :

```json
{
  "url": "https://exemple.com/vin-du-rhone",
  "domain": "exemple.com",
  "lang": "fr",
  "text": "Les vins du RhÃ´ne se caractÃ©risent par...",
  "score_relevance": 0.81
}
```

Ces fichiers JSONL peuvent ensuite servir pour :

* entraÃ®nement dâ€™un **modÃ¨le encodeur-dÃ©codeur spÃ©cialisÃ© "vin"**,
* construction dâ€™un corpus multilingue,
* segmentation / labeling automatiques,
* gÃ©nÃ©ration de datasets QA / rÃ©sumÃ© / classification.

---

# ğŸ”§ Points dâ€™extension (IA Ready)

UltimateScraper est conÃ§u pour accueillir des modules IA custom :

### Plug-in model ClfDoc maison

â†’ Filtrage par classification automatique de page (vin / pas vin / type)

### Plug-in NER / Slot Extraction

â†’ Extraction automatique de :

* cÃ©pages
* rÃ©gions
* appellations
* note de dÃ©gustation
* domaine / producteur

### Plug-in MT5 ou Qwen

â†’ RÃ©sumÃ© automatique
â†’ Normalisation du contenu
â†’ GÃ©nÃ©ration de descriptions marketing

---

# ğŸ“ˆ Roadmap

### Phase 1 â€” TerminÃ©

âœ”ï¸ Crawler minimal complet
âœ”ï¸ Filtrage embeddings multilingue
âœ”ï¸ Extraction texte propre (trafilatura)
âœ”ï¸ Gestion seeds + limites + robots.txt
âœ”ï¸ Architecture modulaire clean

### Phase 2 â€” Ã€ venir

ğŸŸ¦ IntÃ©gration de ton modÃ¨le ClfDoc
ğŸŸ¦ Extraction NER vin (SlotNER maison)
ğŸŸ¦ PrÃ©paration automatique de dataset pour modÃ¨le encodeur-dÃ©codeur
ğŸŸ¦ Support Playwright pour JS heavy sites
ğŸŸ¦ Multi-crawler distributed (Ray / multiprocessing)

---

# ğŸ‘¨â€ğŸ’» Auteur & Objectif

Projet initiÃ© par **Paul Berdier**, Data Scientist chez Prooftag.
Fait pour construire un **crawler IA professionnel**, scalable, modulaire, et capable de gÃ©nÃ©rer automatiquement des datasets propres pour entraÃ®ner des modÃ¨les spÃ©cialisÃ©s (vin, cosmÃ©tique, sÃ©curitÃ© produit, etc.).
